# -*- coding: utf-8 -*-
"""FakeJobCode_Real.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14YGSaMWeZoxZx6BEBEzoJKFRcqQpeyrR
"""

##This block is only for access of files using google drive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

#For accessing any file from google drive, first share it for public access. Copy its id from last part of its address. Then specify the two lines below.
downloaded = drive.CreateFile({'id':"1gzi_XR1oSa8LO-UcAWbi8G7bhWyY2TPh"})   # replace the id with id of file you want to access
downloaded.GetContentFile('job_posting.csv')
downloaded = drive.CreateFile({'id':"1vkRc4tYuLHKzpAZIa90RdXaiGY6xutuH"}) 
downloaded.GetContentFile('LinkedIn Job Data.csv') # Downloading Linkedin job data collected from Linkedin using Web Scrapping

import numpy as np
import pandas as pd
import string
import matplotlib.pyplot as plt
#import the data file
filename = 'job_posting.csv' 
df_job = pd.read_csv('job_posting.csv',encoding='latin-1') 
lin_job = pd.read_csv('LinkedIn Job Data.csv', encoding = 'latin-1')

df_job.head()

# Showing some Linkedin job data collected from Linkedin using Web Scrapping
fraudulent = ['0']*len(lin_job)
lin_job['fraudulent'] = fraudulent
lin_job.head()

lin_job.info()

df_job.info()                     #part of initial eda

df_job = df_job.fillna(' ') #Remove all the NAN values
df_job.shape

df_job.isnull().sum() # Shows null values in each column

import seaborn as sn
corr_Mat = df_job.corr()
fig, ax = plt.subplots(figsize=(5,5))  
sn.heatmap(corr_Mat, annot=True, ax= ax)

fig, axes = plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
plt.tight_layout()

df_job["fraudulent"].value_counts().plot(kind='pie', ax=axes[0], labels=['Real Post (95%)', 'Fake Post (5%)'])
temp = df_job["fraudulent"].value_counts()
sn.barplot(temp.index, temp, ax=axes[1])

axes[0].set_ylabel(' ')
axes[1].set_ylabel(' ')
axes[1].set_xticklabels(["Real Post = 17014", "Fake Post = 866"])

axes[0].set_title('Fraud vs Real Job Post Distribution in Dataset', fontsize=13)
axes[1].set_title('Fraud vs Real Job Post Count in Dataset', fontsize=13)

plt.show()                  # histogram of fraudulent values

Info_df = df_job[["job_id","title",	"department", "company_profile", "description", "requirements", "benefits","required_experience","required_education","fraudulent"]]

Info_df.head()

# Concate the text data for preprocessing and modeling
text = Info_df[Info_df.columns[0:-1]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)
text_y = Info_df['fraudulent']
print(len(text))
print(len(text_y))

# Concate the linkedin text data for preprocessing, testing
lin_text = lin_job[lin_job.columns[0:-1]].apply(lambda x: ','.join(x.dropna().astype(str)),axis=1)
lin_y = lin_job['fraudulent']

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def get_top_words(corpus, n=None):
    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

fig, axes = plt.subplots(ncols=1, figsize=(15, 10), dpi=100)
plt.tight_layout()

top_words=get_top_words(text)[:50]
x,y=map(list,zip(*top_words))
sn.barplot(x=y,y=x, color='teal')


axes.set_ylabel(' ')
axes.set_title('Top 50 most common words', fontsize=15)

plt.show()

text[0:50]

from wordcloud import WordCloud, STOPWORDS
import nltk  
#nltk.download('stopwords') 
#from nltk.corpus import stopwords 

#STOPWORDS.add('https')
def wordcloud(text):
  text_words = ' '
  for i in range(int(len(text))):
    #stopwords = set(STOPWORDS) 
    for word in text:
      word = str(word) # typecaste each word to string 
      tokens = word.split() # split the value 
    for i in range(len(tokens)): 
      tokens[i] = tokens[i].lower() # Converts each token into lowercase 
    for words in tokens:
      text_words =  text_words + words + ' '                                                  # stopwords = stopwords,
  wordcloud = WordCloud(width = 3000, height = 3000, background_color ='black', max_words= 200,min_font_size = 10).generate(text_words) 
  plt.figure(figsize = (12, 12), facecolor = 'k', edgecolor = 'k' ) 
  plt.imshow(wordcloud) 
  plt.axis("off") 
  plt.tight_layout(pad = 0) 
  plt.show() 

wordcloud(text[100:300])

text_nf = text[text_y==0]

for i in range(len(text)):
  text_words = (text_nf.str.split(" "))[i]
  if len(text_words) < 2000:
    num_mod[i] = text_words;

fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)
num =text[text_y==1].str.split().map(lambda x: len(x))
ax1.hist(num,bins = 30,color='orangered')
ax1.set_title('Fake Job Post')
ax1.set_xlabel("Total no. of words")
ax1.set_ylabel("No, of job")
num =text[text_y==0].str.split().map(lambda x: len(x))

ax2.hist(num,bins = 30,range=(0,1400))
ax2.set_title('Real Job Post')
ax2.set_xlabel("Total no. of words")
ax2.set_ylabel("No, of job")
fig.suptitle('Words in Description')
plt.show()

len(text)

num_1 =(text[1000:2000])[text_y==1]
wordcloud(num_1)

num_0 =text[text_y==0]
wordcloud(num_0)

#Make text lowercase,remove stopwords,remove text in square brackets,remove links,remove punctuation and remove words containing numbers
import re
import string
def preprocess_text(text):
    text = text.lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

# Applying the cleaning function to both test and training datasets
text = text.apply(lambda x: preprocess_text(x))
text.head(4)

# Removing Stopwords using Gensim
from gensim.parsing.preprocessing import remove_stopwords
text = text.apply(lambda x : remove_stopwords(x))
text.head(4)

# Preprocessing LinkedIn data
lin_text = lin_text.apply(lambda x: preprocess_text(x))
lin_text = lin_text.apply(lambda x : remove_stopwords(x))

# Splitting the data into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(text, text_y, test_size = 0.25,stratify = text_y, random_state = 2)

# Using NLP(GLOVE vectors) to convert words into Nomalised Vectors

!wget http://nlp.stanford.edu/data/glove.6B.zip # Download the zip file

!unzip glove*.zip.1

!ls # Get the exact path of where the embedding vectors are extracted 
!pwd

# Loading the GLOVE vectors
embeddings_dict = {}
with open("glove.6B.200d.txt", 'r') as f: # Loading pretrained vectors
  for line in f: # loop through each line in the file
    values = line.split() # Split the line by every space, into each of its components
    word = values[0] #  Making assumption that the word does not have any spaces in it by setting it equal the first (or zeroth) element of the split line
    vector = np.asarray(values[1:], "float32") # Converting rest of the line into a Numpy array
    embeddings_dict[word] = vector
f.close()
print('Found %s word vectors.' % len(embeddings_dict))

# A function to create a normalized vector for the whole sentence
from nltk.tokenize import word_tokenize
def Nor_vector(data):
    words = str(data).lower()
    words = word_tokenize(words)
    #words = [w for w in words if not w in stopwords]
    words = [w for w in words if w.isalpha()]
    M = []
    for w in words:
        try:
            M.append(embeddings_dict[w])
        except:
            continue
    M = np.array(M)
    v = M.sum(axis=0)
    if type(v) != np.ndarray:
        return np.zeros(200)
    return v / np.sqrt((v ** 2).sum())

import nltk
nltk.download('punkt')
from tqdm import tqdm
x_train = np.array([Nor_vector(x) for x in tqdm(X_train)])
x_test = np.array([Nor_vector(x) for x in tqdm(X_test)])

from sklearn import preprocessing
scale = preprocessing.StandardScaler() # Standardize features by removing the mean and scaling to unit variance using sklearn
x_train = scale.fit_transform(x_train) # Fits transformer to X and y and returns a transformed version of X.
x_test = scale.transform(x_test) # Perform standardization by centering and scaling

# Creating a sequential Neural Network of 2 layers as more layers 
# results in more parameters to be trained which can cause overfitting

from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.normalization import BatchNormalization

# Using Sequential model to train the data
model = Sequential() 

model.add(Dense(250, input_dim=200, activation='relu')) # Using relu as activation function in hidden layers because it's a classification problem
model.add(Dropout(0.25)) # Dropout Regularisation avoids overfitting 
model.add(BatchNormalization()) # Normalizing the layers by adjusting and scaling the activations

model.add(Dense(150, activation='relu'))
model.add(Dropout(0.25))
model.add(BatchNormalization())

model.add(Dense(50, activation='relu'))
model.add(Dropout(0.25))
model.add(BatchNormalization())

model.add(Dense(1))
model.add(Activation('sigmoid')) # Using sigmoid as activation function in output layer because it's a classification problem

# compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Fitting the model
hist_model = model.fit(x_train, y=y_train, batch_size=64, epochs=10, verbose=1, validation_data=(x_test, y_test))

#SCORE EVALUATION
score = model.evaluate(x_test, y_test) 
print('Test loss:', score[0]) 
print('Test accuracy:', score[1])

from sklearn.metrics import roc_auc_score
predictions = model.predict(x_test)
predictions = np.round(predictions).astype(int)
print ("AUC score :", np.round(roc_auc_score(y_test, predictions),5))

from sklearn.metrics import roc_auc_score
predictions = model.predict(x_train)
predictions = np.round(predictions).astype(int)
print ("AUC score :", np.round(roc_auc_score(y_train, predictions),5))

pred = model.predict(x_train) 
count = 0
for i in range(len(x_train)):
  if (pred)[i] > 0.5:
    count +=1
print(count)

# Plot results
import matplotlib.pyplot as plt

accu = hist_model.history['accuracy']
val_acc = hist_model.history['val_accuracy']
loss = hist_model.history['loss']
val_loss = hist_model.history['val_loss']

epochs = range(1, len(accu)+1)

plt.plot(epochs, accu, 'g', label='Training accuracy')
plt.plot(epochs, val_acc, 'r', label='Validation accuracy')
plt.xlabel("No. of epoch")
plt.ylabel("Accuracy(0-1)")
plt.title('Training and validation accuracy')

plt.legend()

plt.figure()

plt.plot(epochs, loss, 'g', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel("No. of epoch")
plt.ylabel("Loss(0-1)")
plt.legend()

plt.show()

lin_x = np.array([Nor_vector(x) for x in tqdm(lin_text)])

scale = preprocessing.StandardScaler() # Standardize features by removing the mean and scaling to unit variance using sklearn
lin_x_train = scale.fit_transform(lin_x) # Fits transformer to X and y and returns a transformed version of X.



lin_pred = model.predict(lin_x_train)
l = len(lin_pred)
count = 0
for i in range(l):
  if lin_pred[i] <= 0.5:
    lin_pred[i] = 0
  else:
    lin_pred[i] = 1
    count += 1

print(count)

lin_Acc = ((l-count)*100)/l

print("The accuracy obtained using the data from Linked is: ",lin_Acc)

